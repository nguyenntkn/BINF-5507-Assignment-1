{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2bab1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler      # Normalization methods\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import scipy.stats as stats\n",
    "\n",
    "# 0. Removing columns with many missing values\n",
    "\n",
    "\n",
    "# 1. Impute Missing Values\n",
    "def impute_missing_values(data, strategy='mean'):\n",
    "    \"\"\"\n",
    "    Fill missing values in the dataset.\n",
    "    :param data: pandas DataFrame\n",
    "    :param strategy: str, imputation method ('mean', 'median', 'mode')\n",
    "    :return: pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Subset feature column\n",
    "    features = data.copy().iloc[:,1:]\n",
    "\n",
    "    # Loop through the features column names that contains numeric values, and fill in missing values in the respective data df column using appropriate method.\n",
    "    for col in features.select_dtypes(include = np.number).columns:\n",
    "        match strategy:\n",
    "            case 'mean':\n",
    "                data[col].fillna(data[col].mean(), inplace = True)\n",
    "            case 'median':\n",
    "                data[col].fillna(data[col].median(), inplace = True)\n",
    "            case 'mode':\n",
    "                data[col].fillna(data[col].mode(), inplace = True)\n",
    "            case _:\n",
    "                print(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# 2. Remove Duplicates\n",
    "def remove_duplicates(data):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from the dataset.\n",
    "    :param data: pandas DataFrame\n",
    "    :return: pandas DataFrame\n",
    "    \"\"\"\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# 3. Normalize Numerical Data\n",
    "def normalize_data(data,method='minmax'):\n",
    "    \"\"\"\n",
    "    Apply normalization to numerical features.\n",
    "    :param data: pandas DataFrame\n",
    "    :param method: str, normalization method ('minmax' (default) or 'standard')\n",
    "    \"\"\"\n",
    "    # Get features column names, and only include numerical columns\n",
    "    features_col = data.copy().select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Defining scaling method\n",
    "    match method:\n",
    "        case 'minmax':\n",
    "            scaler = MinMaxScaler() \n",
    "        case 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        case _:\n",
    "            print(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Apply method to data, only on the selected columns\n",
    "    data[features_col] = scaler.fit_transform(data[features_col])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# 4. Remove Redundant Features   \n",
    "def remove_redundant_features(data, threshold=0.9):\n",
    "    \"\"\"Remove redundant or duplicate columns.\n",
    "    :param data: pandas DataFrame\n",
    "    :param threshold: float, correlation threshold\n",
    "    :return: pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Subset feature columns with numerical values\n",
    "    features = data.select_dtypes(include=np.number).iloc[:,1:]\n",
    "\n",
    "    # On numerical columns, calculate the correlation (Pearson method) using .corr()\n",
    "    # .abs() get absolute value. \n",
    "    corr_matrix = features.corr().abs()\n",
    "\n",
    "    # np.ones() Create a new matrix with the same dimension as the original correlation matrix and fill the values with 1s.\n",
    "    # np.triu() Get upper triangle of the matrix, indicated by 1s and 0s.\n",
    "    # .astype(bool) Convert 0 -> F and 1 -> T.\n",
    "    # corr_matrix.where() To apply selection to original correlation matrix.\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Identify any columns within the upper correlation matrix that has a high correlation value with another column/\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "\n",
    "    # Drop those chosen columns\n",
    "    data = data.drop(columns=to_drop)\n",
    "\n",
    "    return data\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def simple_model(input_data, split_data=True, scale_data=False, print_report=False):\n",
    "    \"\"\"\n",
    "    A simple logistic regression model for target classification.\n",
    "    Parameters:\n",
    "    input_data (pd.DataFrame): The input data containing features and the target variable 'target' (assume 'target' is the first column).\n",
    "    split_data (bool): Whether to split the data into training and testing sets. Default is True.\n",
    "    scale_data (bool): Whether to scale the features using StandardScaler. Default is False.\n",
    "    print_report (bool): Whether to print the classification report. Default is False.\n",
    "    Returns:\n",
    "    None\n",
    "    The function performs the following steps:\n",
    "    1. Removes columns with missing data.\n",
    "    2. Splits the input data into features and target.\n",
    "    3. Encodes categorical features using one-hot encoding.\n",
    "    4. Splits the data into training and testing sets (if split_data is True).\n",
    "    5. Scales the features using StandardScaler (if scale_data is True).\n",
    "    6. Instantiates and fits a logistic regression model.\n",
    "    7. Makes predictions on the test set.\n",
    "    8. Evaluates the model using accuracy score and classification report.\n",
    "    9. Prints the accuracy and classification report (if print_report is True).\n",
    "    \"\"\"\n",
    "\n",
    "    # if there's any missing data, remove the columns\n",
    "    input_data.dropna(inplace=True)\n",
    "\n",
    "    # split the data into features and target\n",
    "    target = input_data.copy()[input_data.columns[0]]\n",
    "    features = input_data.copy()[input_data.columns[1:]]\n",
    "\n",
    "    # if the column is not numeric, encode it (one-hot)\n",
    "    for col in features.columns:\n",
    "        if features[col].dtype == 'object':\n",
    "            features = pd.concat([features, pd.get_dummies(features[col], prefix=col)], axis=1)\n",
    "            features.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, stratify=target, random_state=42)\n",
    "\n",
    "    if scale_data:\n",
    "        # scale the data\n",
    "        X_train = normalize_data(X_train)\n",
    "        X_test = normalize_data(X_test)\n",
    "        \n",
    "    # instantiate and fit the model\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=100, solver='liblinear', penalty='l2', C=1.0)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions and evaluate the model\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    \n",
    "    # if specified, print the classification report\n",
    "    if print_report:\n",
    "        print('Classification Report:')\n",
    "        print(report)\n",
    "        print('Read more about the classification report: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html and https://www.nb-data.com/p/breaking-down-the-classification')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8533333333333334\n",
      "     target                 a         b         c                  d  \\\n",
      "0       0.0    lv hypertrophy  0.610389  0.714286       fixed defect   \n",
      "1       1.0    lv hypertrophy  0.358036  0.795918             normal   \n",
      "2       1.0    lv hypertrophy  0.595613  0.795918  reversable defect   \n",
      "3       0.0            normal  0.301262  0.183673             normal   \n",
      "4       0.0    lv hypertrophy  0.311787  0.265306             normal   \n",
      "..      ...               ...       ...       ...                ...   \n",
      "890     1.0            normal  0.664370  0.510204  reversable defect   \n",
      "891     0.0    lv hypertrophy  0.660077  0.183673             normal   \n",
      "892     1.0  st-t abnormality  0.781702  0.795918  reversable defect   \n",
      "902     1.0    lv hypertrophy  0.000000  0.551020  reversable defect   \n",
      "903     1.0            normal  0.584563  0.571429  reversable defect   \n",
      "\n",
      "                 e         f                g         h      i  ...         l  \\\n",
      "0        Cleveland  0.505577   typical angina  0.386401   True  ...  0.319166   \n",
      "1        Cleveland  0.729423     asymptomatic  0.474295  False  ...  0.412870   \n",
      "2        Cleveland  0.409365     asymptomatic  0.379768  False  ...  0.124036   \n",
      "3        Cleveland  0.603448      non-anginal  0.414594  False  ...  0.387424   \n",
      "4        Cleveland  0.368214  atypical angina  0.338308  False  ...  0.411798   \n",
      "..             ...       ...              ...       ...    ...  ...       ...   \n",
      "890  VA Long Beach  0.626298     asymptomatic  0.402985  False  ...  0.704386   \n",
      "891  VA Long Beach  0.327331      non-anginal  0.398010  False  ...  0.381804   \n",
      "892  VA Long Beach  0.073332     asymptomatic  0.363184  False  ...  0.456451   \n",
      "902  VA Long Beach  0.337199     asymptomatic  0.374793  False  ...  0.746906   \n",
      "903  VA Long Beach  0.387816     asymptomatic  0.336650   True  ...  0.395203   \n",
      "\n",
      "               m         n       s         t      v         w         x  \\\n",
      "0    downsloping  0.777404    Male  0.556818  0.725  0.296233  0.000000   \n",
      "1           flat  0.498693    Male  0.465909  0.800  0.453651  1.000000   \n",
      "2           flat  0.649442    Male  0.590909  0.600  0.444626  0.666667   \n",
      "3    downsloping  0.964459    Male  0.693182  0.650  0.255787  0.000000   \n",
      "4      upsloping  0.893518  Female  0.454545  0.650  0.523153  0.000000   \n",
      "..           ...       ...     ...       ...    ...       ...       ...   \n",
      "890         flat  0.602107    Male  0.522727  0.620  0.398630  0.222502   \n",
      "891         flat  0.858267    Male  0.409091  0.590  0.318927  0.222502   \n",
      "892         flat  0.602107    Male  0.522727  0.700  0.584643  0.222502   \n",
      "902  downsloping  0.636185    Male  0.488636  0.600  0.695863  0.222502   \n",
      "903         flat  0.416257    Male  0.465909  0.650  0.491947  0.222502   \n",
      "\n",
      "            z      {  \n",
      "0    0.769270  False  \n",
      "1    0.769270   True  \n",
      "2    0.769270   True  \n",
      "3    0.769270  False  \n",
      "4    0.769270  False  \n",
      "..        ...    ...  \n",
      "890  0.907434   True  \n",
      "891  0.769270  False  \n",
      "892  0.769270   True  \n",
      "902  0.926111   True  \n",
      "903  0.769270  False  \n",
      "\n",
      "[372 rows x 22 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4g/rcj0yxyd1rg5h5l6_l1217k00000gn/T/ipykernel_33526/2628944169.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].mean(), inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "# import data_preprocessor as dp\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "messy_data = pd.read_csv('../Data/messy_data.csv')\n",
    "clean_data = messy_data.copy()\n",
    "\n",
    "# 2. Preprocess the data\n",
    "clean_data = impute_missing_values(clean_data, strategy='mean')\n",
    "clean_data = remove_duplicates(clean_data)\n",
    "\n",
    "# clean_data = replace_outlier(clean_data)\n",
    "clean_data = normalize_data(clean_data)\n",
    "clean_data = remove_redundant_features(clean_data)\n",
    "\n",
    "# 3. Save the cleaned dataset\n",
    "clean_data.to_csv('../Data/clean_data.csv', index=False)\n",
    "\n",
    "# 4. Train and evaluate the model\n",
    "simple_model(clean_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pixi Default Environment",
   "language": "python",
   "name": "pixi-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
